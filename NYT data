

import numpy as np
import scipy.io
import time
import matplotlib.pyplot as plt
import pickle
import copy
from scipy import sparse
from collections import Counter


with open('/Users/Edith/Downloads/nyt/vocab.pkl', 'rb') as f:
    words = pickle.load(f) #212,237 unique words


tr_counts = scipy.io.loadmat('/Users/Edith/Downloads/nyt/bow_tr_counts.mat')
tr_tokens = scipy.io.loadmat('/Users/Edith/Downloads/nyt/bow_tr_tokens.mat')

counts = tr_counts.get('counts')[0] #1,368,205 total documents
tok = tr_tokens.get('tokens')[0]#nonzero vocab indices for each document

#new corpus size: number of documents and vocab size
D=1000
V=2000

#turn into 2d array
x= [counts[i][0] for i in range(D)]
tok = [tok[i][0] for i in range(D)]

#flatten x and tok
flatx = [item for sublist in x for item in sublist] 
flattok = [item for sublist in tok for item in sublist]

#sort words by frequency 
all = []
for i in range(len(flatx)):
    for j in range(flatx[i]):
        all.append(flattok[i])
pairs = Counter(all)

#trim only most V most common words
mostcommon = pairs.most_common(V)
vocab = [p[0] for p in mostcommon]
newtok = [[v if v in vocab else 0 for v in tok[i]] for i in range(D)]

for i in range(D):
    x[i] = [x[i][j] for j in np.array(newtok[i]).nonzero()]

#final data: x is counts, newtok is new tokens vector 
x = np.array(x)
x = [x[i][0] for i in range(D)]

tok = [[vocab.index(v) for v in newtok[i] if v!=0] for i in range(D)]

#for non-ragged arrays
c = np.zeros((D,V))
for i in range(D):
    uniq = 0
    for j in tok[i]:
        c[i][j] = x[i][uniq]
        uniq +=1 

x = np.array(c)

#delete unneeded stuff
del tr_counts
del tr_tokens
del counts
del newtok
del c
del tok




